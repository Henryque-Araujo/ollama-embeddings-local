# 🔠 Gerando Embeddings Localmente com Ollama

Este projeto demonstra como gerar vetores de embeddings a partir de textos simples utilizando o modelo `nomic-embed-text`, executado localmente com a ferramenta Ollama.

Através de um script em Python, o usuário conecta-se ao Ollama via API local, envia textos curtos e recebe vetores numéricos (embeddings) que representam semanticamente cada frase.

---

## 🎯 Objetivo

- Instalar e executar o Ollama localmente.
- Usar o modelo `nomic-embed-text` para gerar embeddings.
- Criar um script em Python para enviar textos e receber os vetores.
- Exibir os resultados no terminal de forma resumida.
- Armazenar os embeddings em memória para possíveis usos futuros (como classificação, busca semântica, etc).

---

## 🧰 Tecnologias Utilizadas

- 🐍 **Python 3.8+**
- 🔗 **Ollama** (API local para modelos de linguagem)
- 📦 **requests** (para chamadas HTTP)
- 🧠 **nomic-embed-text** (modelo de embeddings open-source)

---

## ⚙️ Requisitos

Antes de iniciar, certifique-se de ter os seguintes itens instalados:

- [Python 3.8 ou superior](https://www.python.org/)
- [Ollama](https://ollama.com/download) instalado e funcionando localmente
- Git (opcional, mas recomendado)

---

## 🛠️ Instalação e Execução

### 1. Clone o repositório

### 2. No terminal, use pip install requests

### 3.Após isso, digite "ollama run nomic-embed-text" e dê run!


