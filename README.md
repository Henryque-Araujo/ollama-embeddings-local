# ğŸ”  Gerando Embeddings Localmente com Ollama

Este projeto demonstra como gerar vetores de embeddings a partir de textos simples utilizando o modelo `nomic-embed-text`, executado localmente com a ferramenta Ollama.

AtravÃ©s de um script em Python, o usuÃ¡rio conecta-se ao Ollama via API local, envia textos curtos e recebe vetores numÃ©ricos (embeddings) que representam semanticamente cada frase.

---

## ğŸ¯ Objetivo

- Instalar e executar o Ollama localmente.
- Usar o modelo `nomic-embed-text` para gerar embeddings.
- Criar um script em Python para enviar textos e receber os vetores.
- Exibir os resultados no terminal de forma resumida.
- Armazenar os embeddings em memÃ³ria para possÃ­veis usos futuros (como classificaÃ§Ã£o, busca semÃ¢ntica, etc).

---

## ğŸ§° Tecnologias Utilizadas

- ğŸ **Python 3.8+**
- ğŸ”— **Ollama** (API local para modelos de linguagem)
- ğŸ“¦ **requests** (para chamadas HTTP)
- ğŸ§  **nomic-embed-text** (modelo de embeddings open-source)

---

## âš™ï¸ Requisitos

Antes de iniciar, certifique-se de ter os seguintes itens instalados:

- [Python 3.8 ou superior](https://www.python.org/)
- [Ollama](https://ollama.com/download) instalado e funcionando localmente
- Git (opcional, mas recomendado)

---

## ğŸ› ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### 1. Clone o repositÃ³rio

### 2. No terminal, use pip install requests

### 3.ApÃ³s isso, digite "ollama run nomic-embed-text" e dÃª run!


