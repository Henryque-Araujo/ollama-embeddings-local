# Objetivo:
# Este script conecta-se ao Ollama localmente para gerar embeddings de m√∫ltiplos textos,
# usando o modelo "nomic-embed-text". Ele percorre uma lista de frases, envia cada uma
# para o servidor do Ollama e armazena os vetores gerados em um dicion√°rio estruturado.
# No final, exibe o tamanho e os primeiros valores de cada vetor.

import requests
import json

# Lista de textos para gerar embeddings
textos = [
    "Ollama √© uma ferramenta poderosa para executar modelos localmente.",
    "A intelig√™ncia artificial est√° transformando o mundo.",
    "Python √© uma linguagem popular para ci√™ncia, an√°lise e engenharia de dados.",
    "O futuro da computa√ß√£o envolve modelos cada vez mais eficientes. Devemos aprender a us√°-los!!!",
]

# URL da API local do Ollama
url = "http://localhost:11434/api/embeddings"

# Dicion√°rio para armazenar os resultados
resultados = {}

# Loop pelos textos
for i, texto in enumerate(textos, 1):
    print(f"\nüîπ Gerando embedding para a frase {i}: \"{texto}\"")

    # Envio da requisi√ß√£o ao Ollama com o modelo correto
    resposta = requests.post(
        url,
        json={
            "model": "nomic-embed-text",
            "prompt": texto
        }
    )

    # Verifica se a requisi√ß√£o foi bem-sucedida
    if resposta.status_code == 200:
        dados = resposta.json()
        vetor = dados["embedding"]

        # Armazena no dicion√°rio com a frase como chave
        resultados[texto] = vetor

        # Exibe informa√ß√µes resumidas
        print(f"‚úÖ Vetor gerado com {len(vetor)} dimens√µes.")
        print(f"üìå Primeiros 8 valores: {vetor[:8]}")
    else:
        print(f"‚ùå Erro ao gerar embedding: {resposta.status_code} - {resposta.text}")

# Finaliza√ß√£o
print("\n‚úÖ Todos os embeddings foram gerados com sucesso.")
